{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.loss_history = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "              batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this classifier using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: N x D array of training data. Each training point is a D-dimensional\n",
    "             column.\n",
    "        - y: 1-dimensional array of length N with labels 0-1, for 2 classes.\n",
    "        - learning_rate: (float) learning rate for optimization.\n",
    "        - reg: (float) regularization strength.\n",
    "        - num_iters: (integer) number of steps to take when optimizing\n",
    "        - batch_size: (integer) number of training examples to use at each step.\n",
    "        - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "        Outputs:\n",
    "        A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        # Add a column of ones to X for the bias sake.\n",
    "        X = LogisticRegression.append_biases(X)\n",
    "        num_train, dim = X.shape\n",
    "        if self.w is None:\n",
    "            # lazily initialize weights\n",
    "            self.w = np.random.randn(dim) * 0.01\n",
    "\n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        self.loss_history = []\n",
    "        for it in xrange(num_iters):\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Sample batch_size elements from the training data and their           #\n",
    "            # corresponding labels to use in this round of gradient descent.        #\n",
    "            # Store the data in X_batch and their corresponding labels in           #\n",
    "            # y_batch; after sampling X_batch should have shape (batch_size, dim)   #\n",
    "            # and y_batch should have shape (batch_size,)                           #\n",
    "            #                                                                       #\n",
    "            # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
    "            # replacement is faster than sampling without replacement.              #\n",
    "            #########################################################################\n",
    "            inds = np.random.choice(X.shape[0], batch_size)\n",
    "            X_batch = X[inds][:]\n",
    "            y_batch = y[inds]\n",
    "            #########################################################################\n",
    "            #                       END OF YOUR CODE                                #\n",
    "            #########################################################################\n",
    "\n",
    "            # evaluate loss and gradient\n",
    "            loss, gradW = self.loss(X_batch, y_batch, reg)\n",
    "            self.loss_history.append(loss)\n",
    "            # perform parameter update\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Update the weights using the gradient and the learning rate.          #\n",
    "            #########################################################################\n",
    "            self.w[inds]-= gradW * learning_rate\n",
    "            #########################################################################\n",
    "            #                       END OF YOUR CODE                                #\n",
    "            #########################################################################\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print 'iteration %d / %d: loss %f' % (it, num_iters, loss)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X, append_bias=False):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict probabilities for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: N x D array of data. Each row is a D-dimensional point.\n",
    "        - append_bias: bool. Whether to append bias before predicting or not.\n",
    "\n",
    "        Returns:\n",
    "        - y_proba: Probabilities of classes for the data in X. y_pred is a 2-dimensional\n",
    "          array with a shape (N, 2), and each row is a distribution of classes [prob_class_0, prob_class_1].\n",
    "        \"\"\"\n",
    "        if append_bias:\n",
    "            X = LogisticRegression.append_biases(X)\n",
    "        ###########################################################################\n",
    "        # TODO:                                                                   #\n",
    "        # Implement this method. Store the probabilities of classes in y_proba.   #\n",
    "        # Hint: It might be helpful to use np.vstack and np.sum                   #\n",
    "        ###########################################################################\n",
    "        X_w = map (lambda x: np.sum(x*self.w), X)\n",
    "        proba_1 = 1. / (1. + np.exp(-X_W))\n",
    "        proba_0 = 1 - proba_1\n",
    "        y_proba = np.hstack(( np.reshape(proba_1, (len(proba_1,1))), np.reshape(proba_0, (len(proba_0,1)))))\n",
    "        ###########################################################################\n",
    "        #                           END OF YOUR CODE                              #\n",
    "        ###########################################################################\n",
    "        return y_proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the ```predict_proba``` method to predict labels for data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: N x D array of training data. Each column is a D-dimensional point.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "          array of length N, and each element is an integer giving the predicted\n",
    "          class.\n",
    "        \"\"\"\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO:                                                                   #\n",
    "        # Implement this method. Store the predicted labels in y_pred.            #\n",
    "        ###########################################################################\n",
    "        y_proba = self.predict_proba(X, append_bias=True)\n",
    "        y_pred = map(lambda yy: 0. if yy < 0.5 else 1., y_proba)\n",
    "    \n",
    "        ###########################################################################\n",
    "        #                           END OF YOUR CODE                              #\n",
    "        ###########################################################################\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        \"\"\"Logistic Regression loss function\n",
    "        Inputs:\n",
    "        - X: N x D array of data. Data are D-dimensional rows\n",
    "        - y: 1-dimensional array of length N with labels 0-1, for 2 classes\n",
    "        Returns:\n",
    "        a tuple of:\n",
    "        - loss as single float\n",
    "        - gradient with respect to weights w; an array of same shape as w\n",
    "        \"\"\"\n",
    "        dw = np.zeros_like(self.w)  # initialize the gradient as zero\n",
    "        loss = 0\n",
    "        # Compute loss and gradient. Your code should not contain python loops.\n",
    "        X_w = map (lambda x: np.sum(x*self.w), X_batch)\n",
    "        loss = -np.sum(y_batch * np.log(1. / (1.+np.exp(-X_w)))) - np.sum((1-y_batch) * np.log(1. - 1. / (1.+np.exp(-X_w))))          \n",
    "        dw = np.dot((y_batch - (1. / (1.+np.exp(X_w)))), X_w)\n",
    "        # Right now the loss is a sum over all training examples, but we want it\n",
    "        # to be an average instead so we divide by num_train.\n",
    "        # Note that the same thing must be done with gradient.\n",
    "        \n",
    "\n",
    "        # Add regularization to the loss and gradient.\n",
    "        # Note that you have to exclude bias term in regularization.\n",
    "\n",
    "\n",
    "        return loss, dw\n",
    "\n",
    "    @staticmethod\n",
    "    def append_biases(X):\n",
    "        return sparse.hstack((X, np.ones(X.shape[0])[:, np.newaxis])).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1],\n",
       "       [3, 4, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((np.array([[1,2],[3,4]]), np.array([1,1])[:, np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3 -5 -7]\n",
      "[ 0.95021293 -0.00673795  0.99908812]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1,2],[3,4],[5,6]])\n",
    "w = np.array([1,-2])\n",
    "res = np.array(map (lambda x: np.sum(x*w), X))\n",
    "print res\n",
    "print np.array([1,0,1]) - np.exp(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[[0,1]][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.73105858,  0.88079708,  0.95257413,  0.98201379,  0.99330715])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1.,2.,3.,4.,5.])\n",
    "1. / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.69314718,  1.09861229,  1.38629436,  1.60943791])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl = np.log(x)\n",
    "xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  1.38629436,  3.29583687,  5.54517744,  8.04718956])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [1., 0., 1., 1.]\n",
    "x = [5., 1., 5., ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.4, 0.8, 0.55]\n",
    "map (lambda yy: 0. if yy < 0.5 else 1., y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = np.reshape(np.array([1,2,3,4,5]), (5,1))\n",
    "y2 = np.reshape(np.array([11,12,13,14,15]), (5,1))\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 11],\n",
       "       [ 2, 12],\n",
       "       [ 3, 13],\n",
       "       [ 4, 14],\n",
       "       [ 5, 15]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((y1, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot([1,1], [[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
